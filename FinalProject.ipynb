{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COGS 108 - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a) Condense dat files: block hash, transaction overview\n",
    "Drop irrelevant columns in bh.dat and tx.dat; store result dataframes to bh.dat and tx.dat.\n",
    "\n",
    "Result:\n",
    "\n",
    "bh.dat: \n",
    "\n",
    "| Block ID | Block Timestamp |\n",
    "\n",
    "tx.dat: \n",
    "\n",
    "|Transaction ID | Block ID |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path for data\n",
    "bh_filepath = 'data/bh.dat'\n",
    "tx_filepath = 'data/tx.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_block_hash = pd.read_csv(bh_filepath, sep = '\\t')\n",
    "df_transaction = pd.read_csv(tx_filepath, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set column names\n",
    "df_block_hash.columns = ['block ID', 'hash', 'timestamp', 'number of transactions']\n",
    "df_transaction.columns = ['transaction ID', 'block ID', 'input count', 'output count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop irrelevant columns\n",
    "df_block_hash.drop(['hash', 'number of transactions'], axis = 1, inplace = True)\n",
    "df_transaction.drop(['input count', 'output count'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check head after drop\n",
    "print(df_block_hash.head())\n",
    "print(df_transaction.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write block hash dataframe to a new dat file\n",
    "bh_drop_filepath = 'data/bh.dat'\n",
    "df_block_hash.to_csv(path_or_buf = bh_drop_filepath, sep = '\\t', index = False, columns = ['block ID', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_drop_filepath = 'data/tx.dat'\n",
    "df_transaction.to_csv(path_or_buf = tx_drop_filepath, sep = '\\t', index = False, \n",
    "                      columns = ['transaction ID', 'block ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b) Condense dat files: transaction input\n",
    "Split txin.dat to small dat files; drop irrelevant columns; store result dataframes to txin_1.dat to txin_11.dat.\n",
    "\n",
    "Result:\n",
    "\n",
    "txin_1.dat - txin_11.dat: \n",
    "\n",
    "| Transaction ID | Address ID |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split transaction_input file\n",
    "!split -l 70000000 data/txin.dat t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set filepath\n",
    "num_txin_file = 11\n",
    "\n",
    "txin_input_filepath = ['txin/taa', 'txin/tab', 'txin/tac', 'txin/tad', 'txin/tae', 'txin/taf', 'txin/tag', 'txin/tah',\n",
    "                      'txin/tai', 'txin/taj', 'txin/tak']\n",
    "txin_output_filepath = ['txin/txin_1.dat', 'txin/txin_2.dat', 'txin/txin_3.dat', 'txin/txin_4.dat', 'txin/txin_5.dat', \n",
    "                        'txin/txin_6.dat', 'txin/txin_7.dat', 'txin/txin_8.dat', 'txin/txin_9.dat', 'txin/txin_10.dat', \n",
    "                        'txin/txin_11.dat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop to drop irrelevant columns in transaction_input\n",
    "for x in range(num_txin_file):\n",
    "    curr_input_filepath = txin_input_filepath[x]\n",
    "    curr_output_filepath = txin_output_filepath[x]\n",
    "    \n",
    "    df_txin = pd.read_csv(curr_input_filepath, sep = '\\t')\n",
    "    \n",
    "    df_txin.columns = ['transaction ID', 'input sequence', 'previous transaction ID', 'previous output sequence', \n",
    "                       'address ID', 'sum']\n",
    "    \n",
    "    df_txin.drop(['input sequence', 'previous transaction ID', 'previous output sequence', 'sum'], \n",
    "                 axis = 1, inplace = True)\n",
    "    \n",
    "    df_txin.to_csv(path_or_buf = curr_output_filepath, sep = '\\t', index = False, \n",
    "                   columns = ['transaction ID', 'address ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c) Condense dat files: transaction output\n",
    "Split txout.dat to small dat files; drop irrelevant columns; store result dataframes to txout_1.dat to txout_12.dat.\n",
    "\n",
    "Result:\n",
    "\n",
    "txout_1.dat - txout_12.dat: \n",
    "\n",
    "| Transaction ID | Address ID |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split transaction_output file\n",
    "!split -l 70000000 data/txout.dat m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set filepath\n",
    "num_txout_file = 12\n",
    "\n",
    "txout_input_filepath = ['txout/maa', 'txout/mab', 'txout/mac', 'txout/mad', 'txout/mae', 'txout/maf', 'txout/mag', \n",
    "                        'txout/mah', 'txout/mai', 'txout/maj', 'txout/mak', 'txout/mal']\n",
    "txout_output_filepath = ['txout/txout_1.dat', 'txout/txout_2.dat', 'txout/txout_3.dat', 'txout/txout_4.dat', \n",
    "                         'txout/txout_5.dat', 'txout/txout_6.dat', 'txout/txout_7.dat', 'txout/txout_8.dat', \n",
    "                         'txout/txout_9.dat', 'txout/txout_10.dat', 'txout/txout_11.dat', 'txout/txout_12.dat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop to drop irrelevant columns in transaction_output\n",
    "for x in range(num_txout_file):\n",
    "    curr_input_filepath = txout_input_filepath[x]\n",
    "    curr_output_filepath = txout_output_filepath[x]\n",
    "    \n",
    "    df_txout = pd.read_csv(curr_input_filepath, sep = '\\t')\n",
    "    \n",
    "    df_txout.columns = ['transaction ID', 'output sequence', 'address ID', 'sum']\n",
    "    \n",
    "    df_txout.drop(['output sequence', 'sum'], axis = 1, inplace = True)\n",
    "    \n",
    "    df_txout.to_csv(path_or_buf = curr_output_filepath, sep = '\\t', index = False, \n",
    "                   columns = ['transaction ID', 'address ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d) Convert UNIX time to standard time\n",
    "Implement a function that converts UNIX to standard time; add a column \"year\" in bh.dat.\n",
    "\n",
    "Before: \n",
    "\n",
    "| Block ID | Block Timestamp |\n",
    "\n",
    "After: \n",
    "\n",
    "| Block ID | Block Timestamp | Month | Year |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1e) Re-split tx_in and tx_out dat files to groups of 2-year time periods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Sample Datasets\n",
    "Data files at this point:\n",
    "\n",
    "block hash: bh.dat\n",
    "\n",
    "transaction overview: tx.dat\n",
    "\n",
    "transaction input: txin_2010.dat, txin_2012.dat, txin_2014.dat, txin_2016.dat\n",
    "\n",
    "transaction output: txout_2010.dat, txout_2012.dat, txout_2014.dat, txout_2016.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a) Load the Data\n",
    "Load data above into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b) Initialize the Dataframe\n",
    "Initialize the following dataframe:\n",
    "\n",
    "| Address ID | Year | NumTX |\n",
    "\n",
    "Address ID: Bitcoin account identifier\n",
    "\n",
    "Year: Year in which the Address ID has its first transaction\n",
    "\n",
    "NumTx: Number of transactions in total belong to that Bitcoin Address ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c) Sample Address ID\n",
    "\n",
    "Sample 100 address IDs in each transaction input/output dat file.\n",
    "\n",
    "Update the following columns in the dataframe:\n",
    "\n",
    "| Address ID | Year |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d) Accumulate Number of Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through all the transaction inputs and outputs; accumulate the numbers of transactions for each sample.\n",
    "\n",
    "Update the following columns in the dataframe:\n",
    "\n",
    "| NumTX |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Normal Distribution and T-test\n",
    "Apply Normal Distributions and T-tests to samples from the following 2 groups:\n",
    "\n",
    "Group 1:\n",
    "\n",
    "Year in 2010 or 2011\n",
    "\n",
    "Group 2:\n",
    "\n",
    "Year in 2016 or 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a) Mean number of transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d) Normal Distribution Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Analysis of Variance/ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
